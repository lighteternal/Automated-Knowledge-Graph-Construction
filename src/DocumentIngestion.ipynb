{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Python Notebook to read all the text files from a text dataset into Neo4j database.\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "1. List all the text files in the sub-directories your dataset.\n",
    "2. Read all the files.\n",
    "3. Create nodes, where n(nodes) = n(files)\n",
    "4. Dump the text files into individual nodes where every node is a document using Graphaware's NLP pipeline.\n",
    "5. Sample scripts for entity extraction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports<br>glob --> for iterating through the folders and sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the path for the files, the wildcards at the end of the path denote that all the files from all the subdirectories from the bbc folder will be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/your/path/here/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glob will help iterate through the entire folder path, allowing wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"neo4j\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate(\"localhost:7474\", \"neo4j\", \"neo4j\")\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")\n",
    "\n",
    "#graph = Graph(host='localhost', user='neo4j',password='password')\n",
    "tx = graph.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d2caff1:Article {Text:\"Data growth and availability as well as data democratization have radically changed data exploration in the last 10 years. Many different data sets, generated by users, systems and sensors, are continuously being collected. These data sets contain information about scientific experiments, health, energy, education etc., and they are highly heterogeneous in nature, ranging from highly structured data in tabular form to unstructured text, images or videos. Furthermore, especially online content, is no longer the purview of large organizations. Open data repositories are made public and can benefit more types of users, from analysts exploring data sets for insight, scientists looking for patterns, to dashboard interactors and consumers looking for information. As a result, the benefit of data exploration becomes increasingly more prominent. However, the volume and complexity of data make it difficult for most users to access data in an easy way.\\n\\nIn this project we propose INODE – Intelligent Open Data Exploration. The core principle of INODE is that users should interact with data in a more dialectic and intuitive way similar to a dialog with a human. To achieve this principle, INODE will offer a suite of agile, fit-for-purpose and sustainable services for exploration of open data sets that help users  link and leverage multiple datasets,  access and search data using natural language, using examples and using analytics, get guidance from the system in understanding the data and formulating the right queries, and  explore data and discover new insights through visualizations.\\n\\nOur service offering is formed by and will initially respond to the needs of large and diverse scientific communities brought by our three use case providers: Cancer Biomarker Research - SIB Swiss Institute of Bioinformatics, Switzerland, Research and Innovation Policy Making - SIRIS, Spain, and  Astrophysics - Max Planck Institute for Extraterrestrial Physics, Germany. \\n\",path:\"/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/siris/inode.txt\"})\n",
      "(c3f4f9c:Article {Text:\"TRESSPASS: robusT Risk basEd Screening and alert System for PASSengers and luggage\\n\\nWith regards to modalities, TRESSPASS project includes air, maritime and land (including car and train) border crossing points, and specifically also travel routes that combine different modalities. It excludes border crossings outside of border crossing points, such as happens with boats of refugees on the Mediterranean. With regards to threats, this includes smuggling, irregular immigration, cross border crime, and terrorism, including threats to the transport itself (so, including e.g. aviation security – per the topic text). It excludes other threats such as posed by state-actors. This proposal includes all tiers of the four-tier access model: measures undertaken in, or jointly with third countries or service providers;  cooperation with neighbouring countries; border control and counter-smuggling measures, and control measures within the area of free movement.\\n\\nTRESSPASS will develop a single cohesive risk-based border management concept, develop three pivoting pilot demonstrators, demonstrate the validity of the single cohesive risk-based border management concept by using red teaming and simulations, prepare for the further development of this concept beyond this project by linking to other known risk-based border management projects (in- and outside EU, within EU research frameworks and on national levels), and describe how their results contribute to a single cohesive risk-based border management concept \\n\",path:\"/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/siris/tresspass.txt\"})\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.glob(folder_path):\n",
    "    with open(filename, 'r') as f: \n",
    "        file_contents = f.read() \n",
    "        Nodes = Node(\"Article\",Text=str(file_contents),path=filename)\n",
    "        print(Nodes)\n",
    "        graph.create(Nodes)\n",
    "        tx.merge(Nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALL ga.nlp.processor.addPipeline({\n",
    "name:\"pipeline\",\n",
    "textProcessor: 'com.graphaware.nlp.processor.stanford.ee.processor.EnterpriseStanfordTextProcessor',\n",
    "processingSteps: {tokenize:true, ner:true, dependencies:true, relations:true, open:true, sentiment:true}\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL ga.nlp.processor.addPipeline({\n",
    "    name: 'pipeline2', \n",
    "    textProcessor: 'com.graphaware.nlp.processor.stanford.StanfordTextProcessor', \n",
    "    processingSteps: {tokenizerAndSentiment:true, ner: true, dependency: true}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL apoc.periodic.iterate(\n",
    "'MATCH (n:Article) RETURN n',\n",
    "'CALL ga.nlp.annotate({\n",
    "        \ttext: n.Text,\n",
    "        \tid: id(n),\n",
    "        \tpipeline: \"pipeline2\",\n",
    "        \tcheckLanguage:false\n",
    "})\n",
    "YIELD result MERGE (n)-[:HAS_ANNOTATED_TEXT]->(result)',\n",
    "{batchSize:1, iterateList:false})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']]\n",
    "RETURN DISTINCT s.value, v.value, o.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']] AND abs(v.startPosition-s.endPosition)<10 AND abs(o.startPosition-v.endPosition)<10 \n",
    "RETURN DISTINCT s.value, v.value, o.value, v.startPosition-s.endPosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(co:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "\n",
    "\n",
    "RETURN se.text as Text, s.value as Subject, v.value as Predicate, am.value as Desc1, nm.value as Desc2,co.value as Desc3,  apr.value as Prop,  o.value as Object LIMIT 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
