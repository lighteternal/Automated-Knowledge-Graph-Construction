{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Python Notebook to read all the text files from a text dataset into Neo4j database (syntactilally-based, SVO)\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "1. List all the text files in the sub-directories your dataset.\n",
    "2. Read all the files.\n",
    "3. Create nodes, where n(nodes) = n(files)\n",
    "4. Dump the text files into individual nodes where every node is a document using Graphaware's NLP pipeline.\n",
    "5. Sample scripts for entity extraction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports<br>glob --> for iterating through the folders and sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the path for the files, the wildcards at the end of the path denote that all the files from all the subdirectories from the bbc folder will be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/biomarker/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glob will help iterate through the entire folder path, allowing wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"1qazxsw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate(\"localhost:7474\", \"neo4j\", \"1qazxsw2\")\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")\n",
    "\n",
    "#graph = Graph(host='localhost', user='neo4j',password='password')\n",
    "tx = graph.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(folder_path):\n",
    "    with open(filename, 'r') as f: \n",
    "        file_contents = f.read() \n",
    "        Nodes = Node(\"Article\",Text=str(file_contents),path=filename)\n",
    "        print(Nodes)\n",
    "        graph.create(Nodes)\n",
    "        tx.merge(Nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALL ga.nlp.processor.addPipeline({\n",
    "name:\"pipeline\",\n",
    "textProcessor: 'com.graphaware.nlp.processor.stanford.ee.processor.EnterpriseStanfordTextProcessor',\n",
    "processingSteps: {tokenize:true, ner:true, dependencies:true, relations:true, open:true, sentiment:true}\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL ga.nlp.processor.addPipeline({\n",
    "    name: 'pipeline2', \n",
    "    textProcessor: 'com.graphaware.nlp.processor.stanford.StanfordTextProcessor', \n",
    "    processingSteps: {tokenizerAndSentiment:true, ner: true, dependency: true}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL apoc.periodic.iterate(\n",
    "'MATCH (n:Article) RETURN n',\n",
    "'CALL ga.nlp.annotate({\n",
    "        \ttext: n.Text,\n",
    "        \tid: id(n),\n",
    "        \tpipeline: \"pipeline2\",\n",
    "        \tcheckLanguage:false\n",
    "})\n",
    "YIELD result MERGE (n)-[:HAS_ANNOTATED_TEXT]->(result)',\n",
    "{batchSize:1, iterateList:false})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']]\n",
    "RETURN DISTINCT s.value, v.value, o.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']] AND abs(v.startPosition-s.endPosition)<10 AND abs(o.startPosition-v.endPosition)<10 \n",
    "RETURN DISTINCT s.value, v.value, o.value, v.startPosition-s.endPosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(co:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "\n",
    "\n",
    "RETURN se.text as Text, s.value as Subject, v.value as Predicate, am.value as Desc1, nm.value as Desc2,co.value as Desc3,  apr.value as Prop,  o.value as Object LIMIT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(co:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "\n",
    "CALL apoc.create.relationship(s, toString(v.value), {}, o) YIELD rel\n",
    "\n",
    "RETURN se.text as Text, s.value as Subject, v.value as Predicate, am.value as Desc1, nm.value as Desc2,co.value as Desc3,  apr.value as Prop,  o.value as Object LIMIT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH a = ()-[:explore]-()\n",
    "MATCH b = ()-[:combine]-()\n",
    "MATCH c = ()-[:discover]-()\n",
    "MATCH d = ()-[:changed]-()\n",
    "MATCH e = ()-[:link]-()\n",
    "\n",
    "RETURN a,b,c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "OPTIONAL MATCH (s:TagOccurrence)-[:COMPOUND]-(coms:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(como:TagOccurrence)\n",
    "\n",
    "RETURN se.id as SentenceID, \n",
    "toString(COALESCE(coms.value+' ', ' '))+\n",
    "toString(s.value) as Subject,\n",
    "v.value as Predicate, \n",
    "toString(COALESCE(am.value+' ',' '))+\n",
    "toString(COALESCE(nm.value+' ', ' '))+\n",
    "toString(COALESCE(apr.value+' ',' '))+\n",
    "toString(COALESCE(como.value+' ',' '))+ \n",
    "toString(o.value) as Object \n",
    "LIMIT 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### OpenIE for automated triple extraction based on open information extraction triples\n",
    "\n",
    "--- \n",
    "0. Coref resolution reads a file and substitutes the antecedents in place, producing a resolved text.\n",
    "1. OpenIE will read the text and extract triples using Stanford Core NLP\n",
    "2. The output is a list of dicts containing \"subject-relation-object\" triples.\n",
    "3. Due to the OIE implementation, similar nodes and edges (almost identical meanings) are present, we can remove them see below, but we may also miss information this way.\n",
    "4. Transfer these triples to Neo4j as nodes-edges.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE() as client:\n",
    "    \n",
    "    with open('/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/siris/tresspass.txt', 'r', encoding='utf8') as r:\n",
    "        corpus = r.read().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    triples_corpus = client.annotate(corpus, \n",
    "                            properties={\n",
    "                            \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                            \"outputFormat\": \"json\",\n",
    "                            \"openie.format\": \"ollie\",\n",
    "                            \"openie.resolve_coref\": \"false\",                         \n",
    "                                 })\n",
    "    \n",
    "    print('Corpus: %s [...].' % corpus[0:80])\n",
    "    print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "    for triple in triples_corpus:\n",
    "        print('|-', triple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural coreference resolution https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python\n",
    "#NEEDS SPACY 2.0.13\n",
    "#Don't run this, run the AllenNLP script below instead\n",
    "import spacy\n",
    "nlp = spacy.load('en_coref_md')\n",
    "doc = nlp(corpus)\n",
    "corpus_coref = doc._.coref_resolved\n",
    "triples_corpus = client.annotate(corpus_coref, \n",
    "                    properties={\n",
    "                    \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                    \"outputFormat\": \"json\",\n",
    "                    \"openie.format\": \"ollie\",\n",
    "                    \"openie.resolve_coref\": \"false\",                         \n",
    "                         })\n",
    "\n",
    "print('Corpus: %s [...].' % corpus_coref[0:80])\n",
    "print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "for triple in triples_corpus:\n",
    "    print('|-', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Coref using Allennlp\n",
    "#NEEDS SPACY 2.2.0\n",
    "from allennlp.predictors import CorefPredictor\n",
    "predictor = CorefPredictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")\n",
    "\n",
    "corpus_coref = predictor.coref_resolved(document=corpus)\n",
    "\n",
    "triples_corpus = client.annotate(corpus_coref, \n",
    "                    properties={\n",
    "                    \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                    \"outputFormat\": \"json\",\n",
    "                    \"openie.format\": \"ollie\",\n",
    "                    \"openie.resolve_coref\": \"false\",                         \n",
    "                         })\n",
    "\n",
    "print('Corpus: %s [...].' % corpus_coref[0:80])\n",
    "print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "for triple in triples_corpus:\n",
    "    print('|-', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate triples (i.e. where some objects are subsets of others ) \n",
    "\n",
    "#MUST BE RUN, but not currently used. To use it substitute triples with unique_triples on the code blocks\n",
    "\n",
    "triples = sorted(triples_corpus, key = lambda i: len(i['object']),reverse=True)\n",
    "\n",
    "duplicates= []\n",
    "unique_triples = []\n",
    "c=0\n",
    "for d in triples:\n",
    "    t = tuple(d.items())\n",
    "    if (triples[c][\"relation\"]) not in duplicates:\n",
    "        duplicates.append(triples[c][\"relation\"])\n",
    "        unique_triples.append(d)\n",
    "    c+=1\n",
    "print(\"===============\")\n",
    "\n",
    "print('Number of triples generated by OpenIE:',len(triples))\n",
    "print('Number of triples after removing duplicates:',len(unique_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to Neo4j DB\n",
    "from py2neo import *\n",
    "from neo4j import GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "authenticate(\"localhost:7474\", \"neo4j\", \"1qazxsw2\")\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"1qazxsw2\"))\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes and relations from extracted triples\n",
    "tx = graph.begin()\n",
    "for i in range(len(triples)):\n",
    "    u1 = Node(\"Subject\", name=triples[i]['subject'], corpus='CORDIS', entry='TRESSPASS')\n",
    "    u2 = Node(\"Object\", name=triples[i]['object'],corpus='CORDIS', entry='TRESSPASS')\n",
    "    graph.merge(u1 | u2)\n",
    "    relation = Relationship(u1,triples[i]['relation'], u2)\n",
    "    graph.merge(relation)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes for the Corpus and the Entry if it doesnt exist already (merge instead of create)\n",
    "u1 = Node(\"Entry\", name='TRESSPASS', corpus='CORDIS')\n",
    "u2 = Node(\"Corpus\", name='CORDIS')\n",
    "graph.merge(u1 | u2)\n",
    "relation = Relationship(u2,'has_entry', u1)\n",
    "graph.merge(relation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather all the triples under the same node\n",
    "tx.run(\"MATCH (sub:Subject), (p:Entry {name: 'TRESSPASS' }) WHERE sub.entry IN ['TRESSPASS'] WITH p, COLLECT(sub) AS subs CREATE (p)-[:contains]->(c: Combo {name:'Triples-TRESSPASS'}) FOREACH(s IN subs | CREATE (c)-[:has_triple]->(s)) RETURN *\")\n",
    "tx.process()\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Misc individual scripts for coref resolution (used above)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural coreference resolution https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_coref_md')\n",
    "\n",
    "doc = nlp(corpus)\n",
    "\n",
    "print(doc._.coref_clusters)\n",
    "\n",
    "print(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-reference resolution using AthNLP\n",
    "\n",
    "from allennlp.predictors import CorefPredictor\n",
    "predictor = CorefPredictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")\n",
    "\n",
    "print(predictor.coref_resolved(document=corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-reference resolution using StanfordCoreNLP (TODO)\n",
    "\n",
    "def resolve(corenlp_output):\n",
    "    \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
    "    for coref in corenlp_output['corefs']:\n",
    "        mentions = corenlp_output['corefs'][coref]\n",
    "        antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
    "        for j in range(1, len(mentions)):\n",
    "            mention = mentions[j]\n",
    "            if mention['type'] == 'PRONOMINAL':\n",
    "                # get the attributes of the target mention in the corresponding sentence\n",
    "                target_sentence = mention['sentNum']\n",
    "                target_token = mention['startIndex'] - 1\n",
    "                # transfer the antecedent's word form to the appropriate token in the sentence\n",
    "                corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
    "\n",
    "\n",
    "def print_resolved(corenlp_output):\n",
    "    \"\"\" Print the \"resolved\" output \"\"\"\n",
    "    possessives = ['hers', 'his', 'their', 'theirs']\n",
    "    for sentence in corenlp_output['sentences']:\n",
    "        for token in sentence['tokens']:\n",
    "            output_word = token['word']\n",
    "            # check lemmas as well as tags for possessive pronouns in case of tagging errors\n",
    "            if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
    "                output_word += \"'s\"  # add the possessive morpheme\n",
    "            output_word += token['after']\n",
    "            print(output_word, end='')\n",
    "\n",
    "\n",
    "output = client.annotate(corpus, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
    "print(output)\n",
    "resolve(output)\n",
    "\n",
    "print('Original:', text)\n",
    "print('Resolved: ', end='')\n",
    "print_resolved(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:athnlp]",
   "language": "python",
   "name": "conda-env-athnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
