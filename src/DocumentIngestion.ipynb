{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Python Notebook to read all the text files from a text dataset into Neo4j database (syntactilally-based, SVO)\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "1. List all the text files in the sub-directories your dataset.\n",
    "2. Read all the files.\n",
    "3. Create nodes, where n(nodes) = n(files)\n",
    "4. Dump the text files into individual nodes where every node is a document using Graphaware's NLP pipeline.\n",
    "5. Sample scripts for entity extraction\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports<br>glob --> for iterating through the folders and sub-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the path for the files, the wildcards at the end of the path denote that all the files from all the subdirectories from the bbc folder will be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/biomarker/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glob will help iterate through the entire folder path, allowing wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"1qazxsw2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authenticate(\"localhost:7474\", \"neo4j\", \"1qazxsw2\")\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")\n",
    "\n",
    "#graph = Graph(host='localhost', user='neo4j',password='password')\n",
    "tx = graph.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in glob.glob(folder_path):\n",
    "    with open(filename, 'r') as f: \n",
    "        file_contents = f.read() \n",
    "        Nodes = Node(\"Article\",Text=str(file_contents),path=filename)\n",
    "        print(Nodes)\n",
    "        graph.create(Nodes)\n",
    "        tx.merge(Nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALL ga.nlp.processor.addPipeline({\n",
    "name:\"pipeline\",\n",
    "textProcessor: 'com.graphaware.nlp.processor.stanford.ee.processor.EnterpriseStanfordTextProcessor',\n",
    "processingSteps: {tokenize:true, ner:true, dependencies:true, relations:true, open:true, sentiment:true}\n",
    "}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL ga.nlp.processor.addPipeline({\n",
    "    name: 'pipeline2', \n",
    "    textProcessor: 'com.graphaware.nlp.processor.stanford.StanfordTextProcessor', \n",
    "    processingSteps: {tokenizerAndSentiment:true, ner: true, dependency: true}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "CALL apoc.periodic.iterate(\n",
    "'MATCH (n:Article) RETURN n',\n",
    "'CALL ga.nlp.annotate({\n",
    "        \ttext: n.Text,\n",
    "        \tid: id(n),\n",
    "        \tpipeline: \"pipeline2\",\n",
    "        \tcheckLanguage:false\n",
    "})\n",
    "YIELD result MERGE (n)-[:HAS_ANNOTATED_TEXT]->(result)',\n",
    "{batchSize:1, iterateList:false})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']]\n",
    "RETURN DISTINCT s.value, v.value, o.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH (s:TagOccurrence)<-[]-(a:Sentence)-[]->(v:TagOccurrence),\n",
    "(a)-[]->(o:TagOccurrence)\n",
    "WHERE s.pos IN [['NNP']] AND v.pos IN [['VBZ']] AND o.pos IN [['NN']] AND abs(v.startPosition-s.endPosition)<10 AND abs(o.startPosition-v.endPosition)<10 \n",
    "RETURN DISTINCT s.value, v.value, o.value, v.startPosition-s.endPosition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEO4J scripts (non-Python code)\n",
    "\n",
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(co:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "\n",
    "\n",
    "RETURN se.text as Text, s.value as Subject, v.value as Predicate, am.value as Desc1, nm.value as Desc2,co.value as Desc3,  apr.value as Prop,  o.value as Object LIMIT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(co:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "\n",
    "CALL apoc.create.relationship(s, toString(v.value), {}, o) YIELD rel\n",
    "\n",
    "RETURN se.text as Text, s.value as Subject, v.value as Predicate, am.value as Desc1, nm.value as Desc2,co.value as Desc3,  apr.value as Prop,  o.value as Object LIMIT 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH a = ()-[:explore]-()\n",
    "MATCH b = ()-[:combine]-()\n",
    "MATCH c = ()-[:discover]-()\n",
    "MATCH d = ()-[:changed]-()\n",
    "MATCH e = ()-[:link]-()\n",
    "\n",
    "RETURN a,b,c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH p= (ar:Article)-[:HAS_ANNOTATED_TEXT]->(an:AnnotatedText)-[:CONTAINS_SENTENCE]->(se:Sentence)-[:SENTENCE_TAG_OCCURRENCE]-(s:TagOccurrence)-[:NSUBJ]-(v:TagOccurrence)-[:DOBJ]-(o:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:AMOD]-(am:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:NMOD]-(nm:TagOccurrence)\n",
    "OPTIONAL MATCH (nm:TagOccurrence)-[:APPOS]-(apr:TagOccurrence)\n",
    "OPTIONAL MATCH (s:TagOccurrence)-[:COMPOUND]-(coms:TagOccurrence)\n",
    "OPTIONAL MATCH (o:TagOccurrence)-[:COMPOUND]-(como:TagOccurrence)\n",
    "\n",
    "RETURN se.id as SentenceID, \n",
    "toString(COALESCE(coms.value+' ', ' '))+\n",
    "toString(s.value) as Subject,\n",
    "v.value as Predicate, \n",
    "toString(COALESCE(am.value+' ',' '))+\n",
    "toString(COALESCE(nm.value+' ', ' '))+\n",
    "toString(COALESCE(apr.value+' ',' '))+\n",
    "toString(COALESCE(como.value+' ',' '))+ \n",
    "toString(o.value) as Object \n",
    "LIMIT 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### OpenIE for automated triple extraction based on open information extraction triples\n",
    "\n",
    "--- \n",
    "\n",
    "1. OpenIE will read a file and extract triples using Stanford Core NLP\n",
    "2. The output is a list of dicts containing \"subject-relation-object\" triples.\n",
    "3. Transfer these triples to Neo4j as nodes-edges.\n",
    "4. Due to the OIE implementation, similar nodes and edges (almost identical meanings) are present, we can remove them see below, but we may also miss information this way.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp /home/earendil/stanfordnlp_resources/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-770b57d3fc6b4732.props -preload openie\n",
      "Corpus: TRESSPASS: robusT Risk basEd Screening and alert System for PASSengers and lugga [...].\n",
      "Found 22 triples in the corpus.\n",
      "|- {'subject': 'TRESSPASS project', 'relation': 'includes', 'object': 'air border points'}\n",
      "|- {'subject': 'TRESSPASS project', 'relation': 'includes', 'object': 'air border crossing points'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats of refugees'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats of refugees on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats of refugees on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats of refugees'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'border crossings'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings outside of', 'object': 'border'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats on Mediterranean'}\n",
      "|- {'subject': 'this', 'relation': 'regards to', 'object': 'threats'}\n",
      "|- {'subject': 'other threats', 'relation': 'posed by', 'object': 'state-actors'}\n",
      "|- {'subject': 'threats', 'relation': 'posed by', 'object': 'state-actors'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'other threats posed by state-actors'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'threats posed'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'other threats posed'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'threats posed by state-actors'}\n",
      "|- {'subject': 'other known risk-based border management projects', 'relation': 'is in', 'object': 'describe'}\n",
      "|- {'subject': 'TRESSPASS', 'relation': 'will develop', 'object': 'border management concept'}\n"
     ]
    }
   ],
   "source": [
    "from openie import StanfordOpenIE\n",
    "\n",
    "with StanfordOpenIE() as client:\n",
    "    \n",
    "    with open('/home/earendil/Desktop/ML_playground/Neo4j-NLP/data/siris/tresspass.txt', 'r', encoding='utf8') as r:\n",
    "        corpus = r.read().replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    triples_corpus = client.annotate(corpus, \n",
    "                            properties={\n",
    "                            \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                            \"outputFormat\": \"json\",\n",
    "                            \"openie.format\": \"ollie\",\n",
    "                            \"openie.resolve_coref\": \"false\",                         \n",
    "                                 })\n",
    "    \n",
    "    print('Corpus: %s [...].' % corpus[0:80])\n",
    "    print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "    for triple in triples_corpus:\n",
    "        print('|-', triple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cymem.cymem.Pool has the wrong size, try recompiling. Expected 64, got 48",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-37f990d4718a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Neural coreference resolution https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_coref_md'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcorpus_coref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_resolved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/site-packages/en_coref_md/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0men_coref_md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneuralcoref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralCoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/athnlp/lib/python3.6/site-packages/en_coref_md/neuralcoref/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mneuralcoref\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralCoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mcymem.pxd\u001b[0m in \u001b[0;36minit en_coref_md.neuralcoref.neuralcoref\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cymem.cymem.Pool has the wrong size, try recompiling. Expected 64, got 48"
     ]
    }
   ],
   "source": [
    "#Neural coreference resolution https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python\n",
    "#NEEDS SPACY 2.0.13\n",
    "import spacy\n",
    "nlp = spacy.load('en_coref_md')\n",
    "doc = nlp(corpus)\n",
    "corpus_coref = doc._.coref_resolved\n",
    "triples_corpus = client.annotate(corpus_coref, \n",
    "                    properties={\n",
    "                    \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                    \"outputFormat\": \"json\",\n",
    "                    \"openie.format\": \"ollie\",\n",
    "                    \"openie.resolve_coref\": \"false\",                         \n",
    "                         })\n",
    "\n",
    "print('Corpus: %s [...].' % corpus_coref[0:80])\n",
    "print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "for triple in triples_corpus:\n",
    "    print('|-', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/earendil/anaconda3/envs/athnlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/earendil/anaconda3/envs/athnlp/lib/python3.6/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:56: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n",
      "/tmp/pip-req-build-ocx5vxk7/aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "WARNING:allennlp.models.model:Encountered the antecedent_indices key in the model's return dictionary which couldn't be split by the batch size. Key will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: TRESSPASS: robusT Risk basEd Screening and alert System for PASSengers and lugga [...].\n",
      "Found 22 triples in the corpus.\n",
      "|- {'subject': 'TRESSPASS project', 'relation': 'includes', 'object': 'air border points'}\n",
      "|- {'subject': 'TRESSPASS project', 'relation': 'includes', 'object': 'air border crossing points'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats of refugees'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats of refugees on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats of refugees on Mediterranean'}\n",
      "|- {'subject': 'It', 'relation': 'happens with', 'object': 'boats of refugees'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'border crossings'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings outside of', 'object': 'border'}\n",
      "|- {'subject': 'It', 'relation': 'excludes border crossings', 'object': 'happens with boats on Mediterranean'}\n",
      "|- {'subject': 'this', 'relation': 'regards to', 'object': 'threats'}\n",
      "|- {'subject': 'other threats', 'relation': 'posed by', 'object': 'state-actors'}\n",
      "|- {'subject': 'threats', 'relation': 'posed by', 'object': 'state-actors'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'other threats posed by state-actors'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'threats posed'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'other threats posed'}\n",
      "|- {'subject': 'It', 'relation': 'excludes', 'object': 'threats posed by state-actors'}\n",
      "|- {'subject': 'other known risk-based border management projects', 'relation': 'is in', 'object': 'describe'}\n",
      "|- {'subject': 'TRESSPASS', 'relation': 'will develop', 'object': 'border management concept'}\n"
     ]
    }
   ],
   "source": [
    "# Neural Coref using Allennlp\n",
    "#NEEDS SPACY 2.2.0\n",
    "from allennlp.predictors import CorefPredictor\n",
    "predictor = CorefPredictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")\n",
    "\n",
    "corpus_coref = predictor.coref_resolved(document=corpus)\n",
    "\n",
    "triples_corpus = client.annotate(corpus_coref, \n",
    "                    properties={\n",
    "                    \"annotators\":\"tokenize,ssplit,pos,lemma,depparse,ner,coref,mention,natlog,openie\",\n",
    "                    \"outputFormat\": \"json\",\n",
    "                    \"openie.format\": \"ollie\",\n",
    "                    \"openie.resolve_coref\": \"false\",                         \n",
    "                         })\n",
    "\n",
    "print('Corpus: %s [...].' % corpus_coref[0:80])\n",
    "print('Found %s triples in the corpus.' % len(triples_corpus))\n",
    "for triple in triples_corpus:\n",
    "    print('|-', triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "Number of triples generated by OpenIE: 22\n",
      "Number of triples after removing duplicates: 9\n"
     ]
    }
   ],
   "source": [
    "#remove duplicate triples (i.e. where some objects are subsets of others ) \n",
    "\n",
    "#MUST BE RUN, but not currently used. To use it substitute triples with unique_triples on the code blocks\n",
    "\n",
    "triples = sorted(triples_corpus, key = lambda i: len(i['object']),reverse=True)\n",
    "\n",
    "duplicates= []\n",
    "unique_triples = []\n",
    "c=0\n",
    "for d in triples:\n",
    "    t = tuple(d.items())\n",
    "    if (triples[c][\"relation\"]) not in duplicates:\n",
    "        duplicates.append(triples[c][\"relation\"])\n",
    "        unique_triples.append(d)\n",
    "    c+=1\n",
    "print(\"===============\")\n",
    "\n",
    "print('Number of triples generated by OpenIE:',len(triples))\n",
    "print('Number of triples after removing duplicates:',len(unique_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to Neo4j DB\n",
    "from py2neo import *\n",
    "from neo4j import GraphDatabase\n",
    "uri = \"bolt://localhost:7687\"\n",
    "authenticate(\"localhost:7474\", \"neo4j\", \"1qazxsw2\")\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"1qazxsw2\"))\n",
    "graph = Graph(\"http://localhost:7474/db/data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes and relations from extracted triples\n",
    "tx = graph.begin()\n",
    "for i in range(len(triples)):\n",
    "    u1 = Node(\"Subject\", name=triples[i]['subject'], corpus='CORDIS', entry='TRESSPASS')\n",
    "    u2 = Node(\"Object\", name=triples[i]['object'],corpus='CORDIS', entry='TRESSPASS')\n",
    "    graph.merge(u1 | u2)\n",
    "    relation = Relationship(u1,triples[i]['relation'], u2)\n",
    "    graph.merge(relation)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nodes for the Corpus and the Entry if it doesnt exist already (merge instead of create)\n",
    "u1 = Node(\"Entry\", name='TRESSPASS', corpus='CORDIS')\n",
    "u2 = Node(\"Corpus\", name='CORDIS')\n",
    "graph.merge(u1 | u2)\n",
    "relation = Relationship(u2,'has_entry', u1)\n",
    "graph.merge(relation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather all the triples under the same node\n",
    "tx.run(\"MATCH (sub:Subject), (p:Entry {name: 'TRESSPASS' }) WHERE sub.entry IN ['TRESSPASS'] WITH p, COLLECT(sub) AS subs CREATE (p)-[:contains]->(c: Combo {name:'Triples-TRESSPASS'}) FOREACH(s IN subs | CREATE (c)-[:has_triple]->(s)) RETURN *\")\n",
    "tx.process()\n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================\n",
    "Misc scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural coreference resolution https://stackoverflow.com/questions/50004797/anaphora-resolution-in-stanford-nlp-using-python\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_coref_md')\n",
    "\n",
    "doc = nlp(corpus)\n",
    "\n",
    "print(doc._.coref_clusters)\n",
    "\n",
    "print(doc._.coref_resolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-reference resolution using AthNLP\n",
    "\n",
    "from allennlp.predictors import CorefPredictor\n",
    "predictor = CorefPredictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz\")\n",
    "\n",
    "print(predictor.coref_resolved(document=corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Co-reference resolution using StanfordCoreNLP (TODO)\n",
    "\n",
    "def resolve(corenlp_output):\n",
    "    \"\"\" Transfer the word form of the antecedent to its associated pronominal anaphor(s) \"\"\"\n",
    "    for coref in corenlp_output['corefs']:\n",
    "        mentions = corenlp_output['corefs'][coref]\n",
    "        antecedent = mentions[0]  # the antecedent is the first mention in the coreference chain\n",
    "        for j in range(1, len(mentions)):\n",
    "            mention = mentions[j]\n",
    "            if mention['type'] == 'PRONOMINAL':\n",
    "                # get the attributes of the target mention in the corresponding sentence\n",
    "                target_sentence = mention['sentNum']\n",
    "                target_token = mention['startIndex'] - 1\n",
    "                # transfer the antecedent's word form to the appropriate token in the sentence\n",
    "                corenlp_output['sentences'][target_sentence - 1]['tokens'][target_token]['word'] = antecedent['text']\n",
    "\n",
    "\n",
    "def print_resolved(corenlp_output):\n",
    "    \"\"\" Print the \"resolved\" output \"\"\"\n",
    "    possessives = ['hers', 'his', 'their', 'theirs']\n",
    "    for sentence in corenlp_output['sentences']:\n",
    "        for token in sentence['tokens']:\n",
    "            output_word = token['word']\n",
    "            # check lemmas as well as tags for possessive pronouns in case of tagging errors\n",
    "            if token['lemma'] in possessives or token['pos'] == 'PRP$':\n",
    "                output_word += \"'s\"  # add the possessive morpheme\n",
    "            output_word += token['after']\n",
    "            print(output_word, end='')\n",
    "\n",
    "\n",
    "output = client.annotate(corpus, properties= {'annotators':'dcoref','outputFormat':'json','ner.useSUTime':'false'})\n",
    "print(output)\n",
    "resolve(output)\n",
    "\n",
    "print('Original:', text)\n",
    "print('Resolved: ', end='')\n",
    "print_resolved(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:athnlp]",
   "language": "python",
   "name": "conda-env-athnlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
